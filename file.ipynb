{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a72991",
   "metadata": {},
   "source": [
    "# Inizializzazione dati\n",
    "Questa cella di inizializzazione carica tutti i file CSV presenti nella cartella `datasets` e crea:\n",
    "\n",
    "- un dizionario `dfs` con chiavi come i nomi dei file (es. `\"characters.csv\"`) e i rispettivi `DataFrame`;\n",
    "- variabili globali con nomi \"puliti\" (es. `characters` per `characters.csv`);\n",
    "- la funzione `show_info(df_or_name, n=5)` per visualizzare forma, tipi, valori mancanti ed una anteprima;\n",
    "- la funzione `reload_datasets()` per ricaricare i file dal disco.\n",
    "\n",
    "Esempi d'uso:\n",
    "\n",
    "- `show_info('characters.csv')` o `show_info(characters)`\n",
    "- `characters.head()`\n",
    "- `dfs['ratings.csv'].shape`\n",
    "- `reload_datasets()` (se modifichi i CSV su disco)\n",
    "\n",
    "Nota: se i file sono molto grandi, puoi modificare `read_csv_safe` o usare `nrows` per caricare un campione durante l'esplorazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "766acf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV trovati: ['datasets\\\\character_anime_works.csv', 'datasets\\\\character_nicknames.csv', 'datasets\\\\characters.csv', 'datasets\\\\details.csv', 'datasets\\\\favs.csv', 'datasets\\\\person_alternate_names.csv', 'datasets\\\\person_anime_works.csv', 'datasets\\\\person_details.csv', 'datasets\\\\person_voice_works.csv', 'datasets\\\\profiles.csv', 'datasets\\\\ratings.csv', 'datasets\\\\recommendations.csv', 'datasets\\\\stats.csv']\n",
      "Skipping datasets\\datasets\\ratings.csv due to read error: Error tokenizing data. C error: out of memory\n",
      "\n",
      "DataFrame caricati:\n",
      " - character_anime_works.csv: (236816, 4)\n",
      " - character_nicknames.csv: (37080, 2)\n",
      " - characters.csv: (209963, 7)\n",
      " - details.csv: (28955, 29)\n",
      " - favs.csv: (4178747, 3)\n",
      " - person_alternate_names.csv: (20465, 2)\n",
      " - person_anime_works.csv: (458091, 3)\n",
      " - person_details.csv: (76699, 10)\n",
      " - person_voice_works.csv: (489516, 5)\n",
      " - profiles.csv: (337155, 10)\n",
      " - recommendations.csv: (105249, 2)\n",
      " - stats.csv: (28955, 27)\n",
      "\n",
      "Variabili disponibili (es.: 'characters' per accedere a 'characters.csv'):\n",
      "['character_anime_works', 'character_nicknames', 'characters', 'details', 'favs', 'person_alternate_names', 'person_anime_works', 'person_details', 'person_voice_works', 'profiles', 'recommendations', 'stats']\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazione: carica tutti i CSV dalla cartella 'datasets' e crea DataFrame accessibili\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "DATA_DIR = Path(\"datasets\")\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Cartella 'datasets' non trovata: {DATA_DIR.resolve()}\")\n",
    "\n",
    "# Trova tutti i file CSV nella cartella (ricorsivamente) e filtra file di sistema (es. __MACOSX)\n",
    "csv_files = sorted([p for p in DATA_DIR.rglob('*.csv') if p.is_file() and p.suffix.lower() == '.csv' and '__MACOSX' not in p.parts and not p.name.startswith('._')])\n",
    "print(\"CSV trovati:\", [str(p.relative_to(DATA_DIR)) for p in csv_files])\n",
    "\n",
    "\n",
    "def read_csv_safe(path, nrows=None):\n",
    "    \"\"\"Legge un CSV provando pi√π encoding se necessario.\"\"\"\n",
    "    last_exc = None\n",
    "    for enc in (None, \"utf-8\", \"latin1\"):\n",
    "        try:\n",
    "            if enc is None:\n",
    "                return pd.read_csv(path, nrows=nrows, low_memory=False)\n",
    "            else:\n",
    "                return pd.read_csv(path, nrows=nrows, low_memory=False, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "    raise last_exc\n",
    "\n",
    "\n",
    "dfs = {}\n",
    "varnames = []\n",
    "for p in csv_files:\n",
    "    try:\n",
    "        df = read_csv_safe(p)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {p} due to read error: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        df = df.convert_dtypes()\n",
    "    except Exception:\n",
    "        pass\n",
    "    dfs[p.name] = df\n",
    "\n",
    "    # Crea una variabile con nome leggibile senza estensione, es. 'characters'\n",
    "    var_name = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", p.stem.lower())\n",
    "    globals()[var_name] = df\n",
    "    varnames.append(var_name)\n",
    "\n",
    "\n",
    "def show_info(df_or_name, n=5):\n",
    "    \"\"\"Mostra informazioni rapide su un DataFrame (oggetto o nome di file/variabile).\"\"\"\n",
    "    if isinstance(df_or_name, str):\n",
    "        if df_or_name in dfs:\n",
    "            df = dfs[df_or_name]\n",
    "        else:\n",
    "            df = globals().get(df_or_name)\n",
    "    else:\n",
    "        df = df_or_name\n",
    "    if df is None:\n",
    "        print(\"DataFrame non trovato\")\n",
    "        return\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nColumns and dtypes:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nMissing values (%):\")\n",
    "    print((df.isnull().mean() * 100).round(2))\n",
    "    display(df.head(n))\n",
    "\n",
    "\n",
    "def reload_datasets():\n",
    "    \"\"\"Ricarica tutti i CSV (utile se cambiano i file su disco).\"\"\"\n",
    "    global dfs, varnames\n",
    "    dfs = {}\n",
    "    varnames = []\n",
    "    for p in sorted(DATA_DIR.rglob('*.csv')):\n",
    "        if not p.is_file() or p.suffix.lower() != \".csv\":\n",
    "            continue\n",
    "        try:\n",
    "            df = read_csv_safe(p)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {p} due to read error: {e}\")\n",
    "            continue\n",
    "        try:\n",
    "            df = df.convert_dtypes()\n",
    "        except Exception:\n",
    "            pass\n",
    "        dfs[p.name] = df\n",
    "        var_name = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", p.stem.lower())\n",
    "        globals()[var_name] = df\n",
    "        varnames.append(var_name)\n",
    "    print(\"Ricaricati:\", [k for k in dfs.keys()])\n",
    "\n",
    "# Riepilogo iniziale\n",
    "print(\"\\nDataFrame caricati:\")\n",
    "for name, df in dfs.items():\n",
    "    print(f\" - {name}: {df.shape}\")\n",
    "print(\"\\nVariabili disponibili (es.: 'characters' per accedere a 'characters.csv'):\")\n",
    "print(varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829ce3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\Utente\\Desktop\\STUDIO\\Terzo anno\\1 - Human computer interaction\\Lab\n",
      "Files in CWD: ['Assignment.pdf', 'datasets', 'datasets.zip', 'file.ipynb', 'HCI Assignment 2025-2026.docx', 'Self Assessment Form - HCI.xlsx']\n",
      "DATA_DIR resolved: C:\\Users\\Utente\\Desktop\\STUDIO\\Terzo anno\\1 - Human computer interaction\\Lab\\datasets\n",
      "DATA_DIR exists: True\n",
      "Contents of DATA_DIR: ['__MACOSX', 'datasets']\n"
     ]
    }
   ],
   "source": [
    "# Verifica working directory e contenuto della cartella 'datasets'\n",
    "from pathlib import Path\n",
    "p = Path.cwd()\n",
    "print(\"Current working directory:\", p)\n",
    "print(\"Files in CWD:\", [x.name for x in p.iterdir()])\n",
    "\n",
    "data_dir = Path(\"datasets\")\n",
    "print(\"DATA_DIR resolved:\", data_dir.resolve())\n",
    "print(\"DATA_DIR exists:\", data_dir.exists())\n",
    "if data_dir.exists():\n",
    "    print(\"Contents of DATA_DIR:\", [x.name for x in sorted(data_dir.iterdir())])\n",
    "else:\n",
    "    # Cerca 'datasets' nelle directory superiori\n",
    "    found = []\n",
    "    for d in [p] + list(p.parents):\n",
    "        cand = d / \"datasets\"\n",
    "        if cand.exists():\n",
    "            found.append(str(cand))\n",
    "    print(\"Found 'datasets' at:\", found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f5a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files contenenti 'ratings': ['datasets\\\\datasets\\\\ratings.csv']\n",
      "Ispeziono: datasets\\datasets\\ratings.csv\n",
      "Header columns: ['username', 'anime_id', 'status', 'score', 'is_rewatching', 'num_watched_episodes']\n",
      "Sample shape: (100000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>status</th>\n",
       "      <th>score</th>\n",
       "      <th>is_rewatching</th>\n",
       "      <th>num_watched_episodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--------788</td>\n",
       "      <td>30276</td>\n",
       "      <td>watching</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--------788</td>\n",
       "      <td>28851</td>\n",
       "      <td>completed</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--------788</td>\n",
       "      <td>41168</td>\n",
       "      <td>completed</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--------788</td>\n",
       "      <td>22199</td>\n",
       "      <td>completed</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--------788</td>\n",
       "      <td>16498</td>\n",
       "      <td>completed</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username  anime_id     status  score  is_rewatching  \\\n",
       "0  --------788     30276   watching      7            0.0   \n",
       "1  --------788     28851  completed      7            0.0   \n",
       "2  --------788     41168  completed      7            0.0   \n",
       "3  --------788     22199  completed     10            0.0   \n",
       "4  --------788     16498  completed     10            0.0   \n",
       "\n",
       "   num_watched_episodes  \n",
       "0                     3  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                    24  \n",
       "4                    25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stima righe: 124298357\n"
     ]
    }
   ],
   "source": [
    "# Helpers per lavorare con CSV grandi e ispezionare 'ratings.csv' (se presente)\n",
    "def count_csv_rows(path):\n",
    "    \"\"\"Conta il numero di righe in modo streaming (meno memoria).\"\"\"\n",
    "    cnt = 0\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for _ in f:\n",
    "            cnt += 1\n",
    "    return max(0, cnt - 1)  # esclude header\n",
    "\n",
    "print(\"CSV files contenenti 'ratings':\", [str(p) for p in csv_files if 'ratings' in p.name.lower()])\n",
    "ratings_paths = [p for p in csv_files if 'ratings' in p.name.lower()]\n",
    "if ratings_paths:\n",
    "    p = ratings_paths[0]\n",
    "    print(\"Ispeziono:\", p)\n",
    "    try:\n",
    "        print(\"Header columns:\", pd.read_csv(p, nrows=0).columns.tolist())\n",
    "    except Exception as e:\n",
    "        print(\"Impossibile leggere l'header:\", e)\n",
    "    try:\n",
    "        sample = pd.read_csv(p, nrows=100000, low_memory=False)\n",
    "        print(\"Sample shape:\", sample.shape)\n",
    "        display(sample.head())\n",
    "    except Exception as e:\n",
    "        print(\"Lettura del campione fallita:\", e)\n",
    "    try:\n",
    "        rows = count_csv_rows(p)\n",
    "        print(\"Stima righe:\", rows)\n",
    "    except Exception as e:\n",
    "        print(\"Counting rows failed:\", e)\n",
    "else:\n",
    "    print(\"Nessun file 'ratings' trovato nei CSV caricati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestione di 'ratings.csv' molto grande: campione e funzioni per processare a chunk\n",
    "ratings_paths = [p for p in csv_files if 'ratings' in p.name.lower()]\n",
    "ratings_path = ratings_paths[0] if ratings_paths else None\n",
    "\n",
    "if ratings_path is not None:\n",
    "    print(\"Ratings path:\", ratings_path)\n",
    "    try:\n",
    "        ratings_sample = pd.read_csv(ratings_path, nrows=200000, low_memory=False)\n",
    "        try:\n",
    "            ratings_sample = ratings_sample.convert_dtypes()\n",
    "        except Exception:\n",
    "            pass\n",
    "        globals()['ratings_sample'] = ratings_sample\n",
    "        dfs['ratings_sample.csv'] = ratings_sample\n",
    "        print(\"ratings_sample caricato:\", ratings_sample.shape)\n",
    "    except Exception as e:\n",
    "        print(\"Impossibile caricare il sample di ratings:\", e)\n",
    "\n",
    "    def ratings_agg_by_anime(chunksize=1_000_000):\n",
    "        \"\"\"Esempio: calcola somma e conteggi 'score' per 'anime_id' senza caricare tutto in memoria.\"\"\"\n",
    "        sums = {}\n",
    "        counts = {}\n",
    "        usecols = ['anime_id', 'score']\n",
    "        for chunk in pd.read_csv(ratings_path, usecols=usecols, chunksize=chunksize, low_memory=False):\n",
    "            chunk = chunk.dropna(subset=['anime_id', 'score'])\n",
    "            ch = chunk.groupby('anime_id')['score'].agg(['sum', 'count']).reset_index()\n",
    "            for _, row in ch.iterrows():\n",
    "                aid = int(row['anime_id'])\n",
    "                sums[aid] = sums.get(aid, 0) + float(row['sum'])\n",
    "                counts[aid] = counts.get(aid, 0) + int(row['count'])\n",
    "        import pandas as _pd\n",
    "        df = _pd.DataFrame({'anime_id': list(sums.keys()), 'sum': list(sums.values()), 'count': [counts[k] for k in sums.keys()]})\n",
    "        df['avg_score'] = df['sum'] / df['count']\n",
    "        return df.sort_values('count', ascending=False)\n",
    "\n",
    "else:\n",
    "    print(\"Nessun file 'ratings' trovato per creare sample o funzioni\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
